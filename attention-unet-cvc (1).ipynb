{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:51:55.870556Z","iopub.execute_input":"2023-08-31T19:51:55.870873Z","iopub.status.idle":"2023-08-31T19:51:56.397052Z","shell.execute_reply.started":"2023-08-31T19:51:55.870843Z","shell.execute_reply":"2023-08-31T19:51:56.396111Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:51:56.399182Z","iopub.execute_input":"2023-08-31T19:51:56.399756Z","iopub.status.idle":"2023-08-31T19:52:05.531427Z","shell.execute_reply.started":"2023-08-31T19:51:56.399714Z","shell.execute_reply":"2023-08-31T19:52:05.530313Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# loading dataset\n\ndef load_data(path , split = 0.1):\n    \n    images = sorted(glob(os.path.join(path , \"PNG/Original/*\")))\n    \n    masks = sorted(glob(os.path.join(path , \"PNG/Ground Truth/*\")))\n    \n    total_size = len(images)\n    valid_size = int(split * total_size)\n    test_size = int(split * total_size)\n    \n    # 612 total \n    # 61 test\n    # 61 validate\n    \n    train_x , valid_x = train_test_split(images , test_size = valid_size, random_state = 42)\n    \n    train_y , valid_y = train_test_split(masks , test_size = valid_size, random_state = 42)\n    \n    \n    train_x , test_x = train_test_split(train_x , test_size = test_size, random_state = 42)\n    \n    train_y , test_y = train_test_split(train_y , test_size = test_size, random_state = 42)\n    \n    return (train_x , train_y ) , (valid_x , valid_y) , (test_x , test_y)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-31T19:52:05.533264Z","iopub.execute_input":"2023-08-31T19:52:05.534051Z","iopub.status.idle":"2023-08-31T19:52:05.542474Z","shell.execute_reply.started":"2023-08-31T19:52:05.534010Z","shell.execute_reply":"2023-08-31T19:52:05.541389Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_image(path):\n    \n    path = path.decode()\n    x = cv2.imread(path , cv2.IMREAD_COLOR)\n    x = cv2.resize(x , (256, 256))\n    x = x/255.0\n    # size is 256, 256 , 3\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:05.544183Z","iopub.execute_input":"2023-08-31T19:52:05.544540Z","iopub.status.idle":"2023-08-31T19:52:05.552915Z","shell.execute_reply.started":"2023-08-31T19:52:05.544506Z","shell.execute_reply":"2023-08-31T19:52:05.551764Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_mask(path):\n    \n    path = path.decode()\n    x = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x , (256, 256))\n    x = x/255.0\n    # size is 256, 256\n    x = np.expand_dims(x, axis = -1)\n    # size is 256, 256 , 1\n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:05.556473Z","iopub.execute_input":"2023-08-31T19:52:05.556815Z","iopub.status.idle":"2023-08-31T19:52:05.563238Z","shell.execute_reply.started":"2023-08-31T19:52:05.556783Z","shell.execute_reply":"2023-08-31T19:52:05.562148Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def tf_parse (x,y) :\n    \n    def _parse(x,y):\n        \n        x = read_image(x)\n        y = read_mask(y)\n        \n        return x,y\n    x,y = tf.numpy_function(_parse , [x,y] , [tf.float64 , tf.float64])\n    \n    x.set_shape([256,256,3])\n    y.set_shape([256,256,1])\n    \n    return x,y","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:05.564755Z","iopub.execute_input":"2023-08-31T19:52:05.565179Z","iopub.status.idle":"2023-08-31T19:52:05.572352Z","shell.execute_reply.started":"2023-08-31T19:52:05.565147Z","shell.execute_reply":"2023-08-31T19:52:05.571307Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def tf_dataset(x,y,batch = 8):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:05.574104Z","iopub.execute_input":"2023-08-31T19:52:05.574558Z","iopub.status.idle":"2023-08-31T19:52:05.582580Z","shell.execute_reply.started":"2023-08-31T19:52:05.574521Z","shell.execute_reply":"2023-08-31T19:52:05.581607Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    path = \"/kaggle/input/cvcclinicdb\"\n    (train_x , train_y ) , (valid_x , valid_y) , (test_x , test_y) = load_data(path,0.1)\n    \n    # 612 total \n    # 61 test\n    # 61 validate\n    # 490 train\n    \n    ds = tf_dataset(test_x , test_y)\n    \n    for x, y in ds:\n        \n        print(x.shape , y.shape) # batch of 8 images with 8 img and 8 masks\n        break","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:05.584795Z","iopub.execute_input":"2023-08-31T19:52:05.586012Z","iopub.status.idle":"2023-08-31T19:52:09.187251Z","shell.execute_reply.started":"2023-08-31T19:52:05.585976Z","shell.execute_reply":"2023-08-31T19:52:09.186230Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(8, 256, 256, 3) (8, 256, 256, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model\n\ndef conv_block(x, num_filters):\n    x = L.Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Activation(\"relu\")(x)\n \n    x = L.Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Activation(\"relu\")(x)\n \n    return x\n\ndef encoder_block(x, num_filters):\n    x = conv_block(x, num_filters)\n    p = L.MaxPool2D((2, 2))(x)\n    return x, p\n\ndef attention_gate(g, s, num_filters):\n    Wg = L.Conv2D(num_filters, 1, padding=\"same\")(g)\n    Wg = L.BatchNormalization()(Wg)\n \n    Ws = L.Conv2D(num_filters, 1, padding=\"same\")(s)\n    Ws = L.BatchNormalization()(Ws)\n \n    out = L.Activation(\"relu\")(Wg + Ws)\n    out = L.Conv2D(num_filters, 1, padding=\"same\")(out)\n    out = L.Activation(\"sigmoid\")(out)\n \n    return out * s\n\n\ndef decoder_block(x, s, num_filters):\n    x = L.UpSampling2D(interpolation=\"bilinear\")(x)\n    s = attention_gate(x, s, num_filters)\n    x = L.Concatenate()([x, s])\n    x = conv_block(x, num_filters)\n    return x\n\n\ndef attention_unet(input_shape):\n    \"\"\" Inputs \"\"\"\n    inputs = L.Input(input_shape)\n \n    \"\"\" Encoder \"\"\"\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n \n    b1 = conv_block(p3, 512)\n \n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s3, 256)\n    d2 = decoder_block(d1, s2, 128)\n    d3 = decoder_block(d2, s1, 64)\n \n    \"\"\" Outputs \"\"\"\n    outputs = L.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d3)\n \n    \"\"\" Model \"\"\"\n    model = Model(inputs, outputs, name=\"Attention-UNET\")\n    return model\n\nif __name__ == \"__main__\":\n    input_shape = (256, 256, 3)\n    model = attention_unet(input_shape)\n    model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:09.189002Z","iopub.execute_input":"2023-08-31T19:52:09.189371Z","iopub.status.idle":"2023-08-31T19:52:09.966298Z","shell.execute_reply.started":"2023-08-31T19:52:09.189335Z","shell.execute_reply":"2023-08-31T19:52:09.965505Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Model: \"Attention-UNET\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d (Conv2D)                (None, 256, 256, 64  1792        ['input_1[0][0]']                \n                                )                                                                 \n                                                                                                  \n batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d[0][0]']                 \n alization)                     )                                                                 \n                                                                                                  \n activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n                                )                                                                 \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 256, 256, 64  36928       ['activation[0][0]']             \n                                )                                                                 \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 256, 256, 64  256        ['conv2d_1[0][0]']               \n rmalization)                   )                                                                 \n                                                                                                  \n activation_1 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_1[0][0]']  \n                                )                                                                 \n                                                                                                  \n max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['activation_1[0][0]']           \n                                )                                                                 \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 128, 128, 12  73856       ['max_pooling2d[0][0]']          \n                                8)                                                                \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 128, 128, 12  512        ['conv2d_2[0][0]']               \n rmalization)                   8)                                                                \n                                                                                                  \n activation_2 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_2[0][0]']  \n                                8)                                                                \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 128, 128, 12  147584      ['activation_2[0][0]']           \n                                8)                                                                \n                                                                                                  \n batch_normalization_3 (BatchNo  (None, 128, 128, 12  512        ['conv2d_3[0][0]']               \n rmalization)                   8)                                                                \n                                                                                                  \n activation_3 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_3[0][0]']  \n                                8)                                                                \n                                                                                                  \n max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['activation_3[0][0]']           \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 64, 64, 256)  295168      ['max_pooling2d_1[0][0]']        \n                                                                                                  \n batch_normalization_4 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_4[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n activation_4 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_4[0][0]']  \n                                                                                                  \n conv2d_5 (Conv2D)              (None, 64, 64, 256)  590080      ['activation_4[0][0]']           \n                                                                                                  \n batch_normalization_5 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_5[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n activation_5 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_5[0][0]']  \n                                                                                                  \n max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['activation_5[0][0]']           \n                                                                                                  \n conv2d_6 (Conv2D)              (None, 32, 32, 512)  1180160     ['max_pooling2d_2[0][0]']        \n                                                                                                  \n batch_normalization_6 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_6[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n activation_6 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_6[0][0]']  \n                                                                                                  \n conv2d_7 (Conv2D)              (None, 32, 32, 512)  2359808     ['activation_6[0][0]']           \n                                                                                                  \n batch_normalization_7 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_7[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n activation_7 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_7[0][0]']  \n                                                                                                  \n up_sampling2d (UpSampling2D)   (None, 64, 64, 512)  0           ['activation_7[0][0]']           \n                                                                                                  \n conv2d_8 (Conv2D)              (None, 64, 64, 256)  131328      ['up_sampling2d[0][0]']          \n                                                                                                  \n conv2d_9 (Conv2D)              (None, 64, 64, 256)  65792       ['activation_5[0][0]']           \n                                                                                                  \n batch_normalization_8 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_8[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n batch_normalization_9 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_9[0][0]']               \n rmalization)                                                                                     \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 64, 64, 256)  0          ['batch_normalization_8[0][0]',  \n da)                                                              'batch_normalization_9[0][0]']  \n                                                                                                  \n activation_8 (Activation)      (None, 64, 64, 256)  0           ['tf.__operators__.add[0][0]']   \n                                                                                                  \n conv2d_10 (Conv2D)             (None, 64, 64, 256)  65792       ['activation_8[0][0]']           \n                                                                                                  \n activation_9 (Activation)      (None, 64, 64, 256)  0           ['conv2d_10[0][0]']              \n                                                                                                  \n tf.math.multiply (TFOpLambda)  (None, 64, 64, 256)  0           ['activation_9[0][0]',           \n                                                                  'activation_5[0][0]']           \n                                                                                                  \n concatenate (Concatenate)      (None, 64, 64, 768)  0           ['up_sampling2d[0][0]',          \n                                                                  'tf.math.multiply[0][0]']       \n                                                                                                  \n conv2d_11 (Conv2D)             (None, 64, 64, 256)  1769728     ['concatenate[0][0]']            \n                                                                                                  \n batch_normalization_10 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_11[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_10 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_10[0][0]'] \n                                                                                                  \n conv2d_12 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_10[0][0]']          \n                                                                                                  \n batch_normalization_11 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_12[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_11 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_11[0][0]'] \n                                                                                                  \n up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 25  0          ['activation_11[0][0]']          \n                                6)                                                                \n                                                                                                  \n conv2d_13 (Conv2D)             (None, 128, 128, 12  32896       ['up_sampling2d_1[0][0]']        \n                                8)                                                                \n                                                                                                  \n conv2d_14 (Conv2D)             (None, 128, 128, 12  16512       ['activation_3[0][0]']           \n                                8)                                                                \n                                                                                                  \n batch_normalization_12 (BatchN  (None, 128, 128, 12  512        ['conv2d_13[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n batch_normalization_13 (BatchN  (None, 128, 128, 12  512        ['conv2d_14[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n tf.__operators__.add_1 (TFOpLa  (None, 128, 128, 12  0          ['batch_normalization_12[0][0]', \n mbda)                          8)                                'batch_normalization_13[0][0]'] \n                                                                                                  \n activation_12 (Activation)     (None, 128, 128, 12  0           ['tf.__operators__.add_1[0][0]'] \n                                8)                                                                \n                                                                                                  \n conv2d_15 (Conv2D)             (None, 128, 128, 12  16512       ['activation_12[0][0]']          \n                                8)                                                                \n                                                                                                  \n activation_13 (Activation)     (None, 128, 128, 12  0           ['conv2d_15[0][0]']              \n                                8)                                                                \n                                                                                                  \n tf.math.multiply_1 (TFOpLambda  (None, 128, 128, 12  0          ['activation_13[0][0]',          \n )                              8)                                'activation_3[0][0]']           \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 128, 128, 38  0           ['up_sampling2d_1[0][0]',        \n                                4)                                'tf.math.multiply_1[0][0]']     \n                                                                                                  \n conv2d_16 (Conv2D)             (None, 128, 128, 12  442496      ['concatenate_1[0][0]']          \n                                8)                                                                \n                                                                                                  \n batch_normalization_14 (BatchN  (None, 128, 128, 12  512        ['conv2d_16[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_14 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_14[0][0]'] \n                                8)                                                                \n                                                                                                  \n conv2d_17 (Conv2D)             (None, 128, 128, 12  147584      ['activation_14[0][0]']          \n                                8)                                                                \n                                                                                                  \n batch_normalization_15 (BatchN  (None, 128, 128, 12  512        ['conv2d_17[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_15 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_15[0][0]'] \n                                8)                                                                \n                                                                                                  \n up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 12  0          ['activation_15[0][0]']          \n                                8)                                                                \n                                                                                                  \n conv2d_18 (Conv2D)             (None, 256, 256, 64  8256        ['up_sampling2d_2[0][0]']        \n                                )                                                                 \n                                                                                                  \n conv2d_19 (Conv2D)             (None, 256, 256, 64  4160        ['activation_1[0][0]']           \n                                )                                                                 \n                                                                                                  \n batch_normalization_16 (BatchN  (None, 256, 256, 64  256        ['conv2d_18[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n batch_normalization_17 (BatchN  (None, 256, 256, 64  256        ['conv2d_19[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n tf.__operators__.add_2 (TFOpLa  (None, 256, 256, 64  0          ['batch_normalization_16[0][0]', \n mbda)                          )                                 'batch_normalization_17[0][0]'] \n                                                                                                  \n activation_16 (Activation)     (None, 256, 256, 64  0           ['tf.__operators__.add_2[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_20 (Conv2D)             (None, 256, 256, 64  4160        ['activation_16[0][0]']          \n                                )                                                                 \n                                                                                                  \n activation_17 (Activation)     (None, 256, 256, 64  0           ['conv2d_20[0][0]']              \n                                )                                                                 \n                                                                                                  \n tf.math.multiply_2 (TFOpLambda  (None, 256, 256, 64  0          ['activation_17[0][0]',          \n )                              )                                 'activation_1[0][0]']           \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 256, 256, 19  0           ['up_sampling2d_2[0][0]',        \n                                2)                                'tf.math.multiply_2[0][0]']     \n                                                                                                  \n conv2d_21 (Conv2D)             (None, 256, 256, 64  110656      ['concatenate_2[0][0]']          \n                                )                                                                 \n                                                                                                  \n batch_normalization_18 (BatchN  (None, 256, 256, 64  256        ['conv2d_21[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_18 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_18[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_22 (Conv2D)             (None, 256, 256, 64  36928       ['activation_18[0][0]']          \n                                )                                                                 \n                                                                                                  \n batch_normalization_19 (BatchN  (None, 256, 256, 64  256        ['conv2d_22[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_19 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_19[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_23 (Conv2D)             (None, 256, 256, 1)  65          ['activation_19[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 8,143,169\nTrainable params: 8,135,745\nNon-trainable params: 7,424\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport tensorflow.keras.backend as K\n\nfrom sklearn.metrics import jaccard_score,confusion_matrix\n\ndef IoU_coef(y_true, y_pred):\n    \n    print(y_true.shape)\n    print(y_pred.shape)\n    \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n\ndef dice_coef(y_true, y_pred):\n    \n    print(y_true.shape)\n    print(y_pred.shape)\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:09.967400Z","iopub.execute_input":"2023-08-31T19:52:09.967782Z","iopub.status.idle":"2023-08-31T19:52:09.990694Z","shell.execute_reply.started":"2023-08-31T19:52:09.967728Z","shell.execute_reply":"2023-08-31T19:52:09.989897Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:09.991624Z","iopub.execute_input":"2023-08-31T19:52:09.991954Z","iopub.status.idle":"2023-08-31T19:52:09.997277Z","shell.execute_reply.started":"2023-08-31T19:52:09.991903Z","shell.execute_reply":"2023-08-31T19:52:09.996479Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross Entropy\n# Focal \n# Dice \n# Tversky\n# Tversky Focal\n# Combo\n# Unified Focal (Sym)\n# Unified Focal (Asym)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:09.998536Z","iopub.execute_input":"2023-08-31T19:52:09.999002Z","iopub.status.idle":"2023-08-31T19:52:10.005667Z","shell.execute_reply.started":"2023-08-31T19:52:09.998968Z","shell.execute_reply":"2023-08-31T19:52:10.004839Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dice_loss = DiceLoss()\n# binary_focal_loss = BinaryFocalLoss()\n# combo_loss = binary_crossentropy + dice_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:10.007262Z","iopub.execute_input":"2023-08-31T19:52:10.007730Z","iopub.status.idle":"2023-08-31T19:52:10.013648Z","shell.execute_reply.started":"2023-08-31T19:52:10.007698Z","shell.execute_reply":"2023-08-31T19:52:10.012859Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ndef dice_coef(y_true, y_pred, smooth=100):        \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return dice\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:10.018926Z","iopub.execute_input":"2023-08-31T19:52:10.019345Z","iopub.status.idle":"2023-08-31T19:52:10.077083Z","shell.execute_reply.started":"2023-08-31T19:52:10.019311Z","shell.execute_reply":"2023-08-31T19:52:10.068922Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def sym_unified_focal_loss(y_true, y_pred ):\n    \n    axis = identify_axis(y_true.get_shape())\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    \n    def loss_function(y_true,y_pred):\n        weight=0.5\n        delta=0.6 \n        gamma=0.5\n        symmetric_ftl = symmetric_focal_tversky_loss(delta=delta, gamma=gamma)(y_true,y_pred)\n        symmetric_fl = symmetric_focal_loss(delta=delta, gamma=gamma)(y_true,y_pred)\n        if weight is not None:\n            return (weight * symmetric_ftl) + ((1-weight) * symmetric_fl)\n        else:\n            return symmetric_ftl + symmetric_fl\n\n    return loss_function(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:10.078459Z","iopub.execute_input":"2023-08-31T19:52:10.078713Z","iopub.status.idle":"2023-08-31T19:52:10.098561Z","shell.execute_reply.started":"2023-08-31T19:52:10.078690Z","shell.execute_reply":"2023-08-31T19:52:10.093328Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def iou(y_true, y_pred) :\n    \n    def f (y_true, y_pred) :\n        \n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        \n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    \n    return tf.numpy_function(f, [y_true , y_pred] , tf.float32)\n        \n      ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:10.099925Z","iopub.execute_input":"2023-08-31T19:52:10.100332Z","iopub.status.idle":"2023-08-31T19:52:10.106880Z","shell.execute_reply.started":"2023-08-31T19:52:10.100307Z","shell.execute_reply":"2023-08-31T19:52:10.106008Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:45:13.680272Z","iopub.execute_input":"2023-06-24T13:45:13.680685Z","iopub.status.idle":"2023-06-24T13:45:13.701549Z","shell.execute_reply.started":"2023-06-24T13:45:13.680635Z","shell.execute_reply":"2023-06-24T13:45:13.700808Z"}}},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:26:06.269823Z","iopub.execute_input":"2023-06-24T18:26:06.270640Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint , ReduceLROnPlateau , CSVLogger, TensorBoard\nfrom tensorflow.keras.metrics import Recall , Precision\nfrom keras import optimizers\nfrom tensorflow.keras.losses import BinaryFocalCrossentropy\n  \n        \nif __name__ == \"__main__\":\n    \n    \n    np.random.seed(42)\n    tf.random.set_seed(42)\n    batch = 16\n    lr = 1e-2\n    epochs = 100\n    \n    train_dataset= tf_dataset(train_x , train_y, batch = batch)\n    valid_dataset= tf_dataset(valid_x , valid_y, batch = batch)\n    \n    \n    model = attention_unet((256, 256, 3))\n    \n    binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n    opt = tf.keras.optimizers.Adam(lr)\n    \n    metrics = [\"acc\" , Recall() , Precision() , iou , dice_coef]\n    \n    \n    model.compile(loss = binary_cross_entropy , optimizer = opt , metrics = metrics)\n    \n    callbacks  = [\n        ModelCheckpoint(\"/kaggle/working/model.h5\"),\n        ReduceLROnPlateau(monitor = \"val_loss\" , factor = 0.5 , patience = 5),\n        EarlyStopping(monitor = \"val_loss\" , patience = 30 , restore_best_weights = False)\n    ]\n    \n    \n    train_steps = len(train_x) // batch\n    valid_steps = len(valid_x) // batch\n    \n    if len(train_x) % batch != 0 :\n        train_steps+= 1\n        \n    if len(valid_x) % batch != 0 :\n        valid_steps+= 1\n\n\n    model.fit(\n        \n        train_dataset, \n        validation_data = valid_dataset,\n        epochs = epochs,\n        steps_per_epoch = train_steps,\n        validation_steps = valid_steps,\n        callbacks = callbacks,\n        shuffle = False\n    )\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T19:52:10.107959Z","iopub.execute_input":"2023-08-31T19:52:10.108330Z","iopub.status.idle":"2023-08-31T20:11:00.664024Z","shell.execute_reply.started":"2023-08-31T19:52:10.108290Z","shell.execute_reply":"2023-08-31T20:11:00.662997Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/100\n31/31 [==============================] - 61s 908ms/step - loss: 0.3200 - acc: 0.8758 - recall: 0.0695 - precision: 0.1603 - iou: 0.0947 - dice_coef: 0.1726 - val_loss: 5891.2104 - val_acc: 0.9039 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_iou: 1.1023e-20 - val_dice_coef: 0.0011 - lr: 0.0100\nEpoch 2/100\n31/31 [==============================] - 16s 525ms/step - loss: 0.2351 - acc: 0.9038 - recall: 0.0677 - precision: 0.4876 - iou: 0.1369 - dice_coef: 0.2397 - val_loss: 245.2926 - val_acc: 0.1189 - val_recall: 1.0000 - val_precision: 0.0909 - val_iou: 0.0887 - val_dice_coef: 0.1627 - lr: 0.0100\nEpoch 3/100\n31/31 [==============================] - 17s 542ms/step - loss: 0.2284 - acc: 0.9061 - recall: 0.1444 - precision: 0.5407 - iou: 0.1546 - dice_coef: 0.2667 - val_loss: 5.4626 - val_acc: 0.5104 - val_recall: 0.9739 - val_precision: 0.1502 - val_iou: 0.1405 - val_dice_coef: 0.2455 - lr: 0.0100\nEpoch 4/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.2218 - acc: 0.9074 - recall: 0.1975 - precision: 0.5515 - iou: 0.1689 - dice_coef: 0.2883 - val_loss: 2.1738 - val_acc: 0.5736 - val_recall: 0.9203 - val_precision: 0.1625 - val_iou: 0.1512 - val_dice_coef: 0.2615 - lr: 0.0100\nEpoch 5/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.2158 - acc: 0.9100 - recall: 0.2065 - precision: 0.5918 - iou: 0.1792 - dice_coef: 0.3033 - val_loss: 0.7602 - val_acc: 0.7422 - val_recall: 0.8006 - val_precision: 0.2281 - val_iou: 0.1900 - val_dice_coef: 0.3185 - lr: 0.0100\nEpoch 6/100\n31/31 [==============================] - 16s 527ms/step - loss: 0.2088 - acc: 0.9117 - recall: 0.2085 - precision: 0.6206 - iou: 0.1915 - dice_coef: 0.3204 - val_loss: 0.2266 - val_acc: 0.9171 - val_recall: 0.2835 - val_precision: 0.5631 - val_iou: 0.1875 - val_dice_coef: 0.3141 - lr: 0.0100\nEpoch 7/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.2040 - acc: 0.9166 - recall: 0.2761 - precision: 0.6593 - iou: 0.2087 - dice_coef: 0.3448 - val_loss: 0.2720 - val_acc: 0.9134 - val_recall: 0.0422 - val_precision: 0.5834 - val_iou: 0.0836 - val_dice_coef: 0.1545 - lr: 0.0100\nEpoch 8/100\n31/31 [==============================] - 17s 541ms/step - loss: 0.1993 - acc: 0.9156 - recall: 0.2643 - precision: 0.6510 - iou: 0.2150 - dice_coef: 0.3527 - val_loss: 0.2562 - val_acc: 0.9133 - val_recall: 0.0416 - val_precision: 0.5774 - val_iou: 0.0902 - val_dice_coef: 0.1657 - lr: 0.0100\nEpoch 9/100\n31/31 [==============================] - 17s 543ms/step - loss: 0.1926 - acc: 0.9202 - recall: 0.3116 - precision: 0.6902 - iou: 0.2329 - dice_coef: 0.3768 - val_loss: 0.2731 - val_acc: 0.8865 - val_recall: 0.4514 - val_precision: 0.3816 - val_iou: 0.1871 - val_dice_coef: 0.3145 - lr: 0.0100\nEpoch 10/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.1898 - acc: 0.9211 - recall: 0.3417 - precision: 0.6809 - iou: 0.2438 - dice_coef: 0.3908 - val_loss: 0.8234 - val_acc: 0.7031 - val_recall: 0.7364 - val_precision: 0.1923 - val_iou: 0.1656 - val_dice_coef: 0.2831 - lr: 0.0100\nEpoch 11/100\n31/31 [==============================] - 16s 530ms/step - loss: 0.1838 - acc: 0.9234 - recall: 0.3481 - precision: 0.7102 - iou: 0.2565 - dice_coef: 0.4061 - val_loss: 0.8054 - val_acc: 0.7107 - val_recall: 0.8108 - val_precision: 0.2084 - val_iou: 0.1876 - val_dice_coef: 0.3151 - lr: 0.0100\nEpoch 12/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.1716 - acc: 0.9285 - recall: 0.4169 - precision: 0.7275 - iou: 0.2905 - dice_coef: 0.4492 - val_loss: 0.2707 - val_acc: 0.8799 - val_recall: 0.5982 - val_precision: 0.3869 - val_iou: 0.2306 - val_dice_coef: 0.3738 - lr: 0.0050\nEpoch 13/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.1653 - acc: 0.9301 - recall: 0.4341 - precision: 0.7349 - iou: 0.3060 - dice_coef: 0.4677 - val_loss: 0.1853 - val_acc: 0.9319 - val_recall: 0.3269 - val_precision: 0.7705 - val_iou: 0.2424 - val_dice_coef: 0.3899 - lr: 0.0050\nEpoch 14/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.1580 - acc: 0.9335 - recall: 0.4753 - precision: 0.7464 - iou: 0.3263 - dice_coef: 0.4911 - val_loss: 0.2014 - val_acc: 0.9223 - val_recall: 0.5743 - val_precision: 0.5632 - val_iou: 0.2580 - val_dice_coef: 0.4091 - lr: 0.0050\nEpoch 15/100\n31/31 [==============================] - 16s 525ms/step - loss: 0.1578 - acc: 0.9335 - recall: 0.4738 - precision: 0.7477 - iou: 0.3273 - dice_coef: 0.4923 - val_loss: 0.1859 - val_acc: 0.9302 - val_recall: 0.5555 - val_precision: 0.6220 - val_iou: 0.2671 - val_dice_coef: 0.4207 - lr: 0.0050\nEpoch 16/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.1520 - acc: 0.9360 - recall: 0.4958 - precision: 0.7618 - iou: 0.3417 - dice_coef: 0.5084 - val_loss: 0.1793 - val_acc: 0.9300 - val_recall: 0.6362 - val_precision: 0.6040 - val_iou: 0.3091 - val_dice_coef: 0.4712 - lr: 0.0050\nEpoch 17/100\n31/31 [==============================] - 16s 524ms/step - loss: 0.1482 - acc: 0.9384 - recall: 0.5175 - precision: 0.7730 - iou: 0.3568 - dice_coef: 0.5250 - val_loss: 0.1614 - val_acc: 0.9366 - val_recall: 0.5657 - val_precision: 0.6735 - val_iou: 0.3171 - val_dice_coef: 0.4804 - lr: 0.0050\nEpoch 18/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.1434 - acc: 0.9402 - recall: 0.5424 - precision: 0.7752 - iou: 0.3700 - dice_coef: 0.5389 - val_loss: 0.1625 - val_acc: 0.9333 - val_recall: 0.5509 - val_precision: 0.6489 - val_iou: 0.3288 - val_dice_coef: 0.4941 - lr: 0.0050\nEpoch 19/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.1370 - acc: 0.9422 - recall: 0.5643 - precision: 0.7821 - iou: 0.3899 - dice_coef: 0.5601 - val_loss: 0.1805 - val_acc: 0.9234 - val_recall: 0.7495 - val_precision: 0.5549 - val_iou: 0.3567 - val_dice_coef: 0.5253 - lr: 0.0050\nEpoch 20/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.1297 - acc: 0.9457 - recall: 0.5862 - precision: 0.8050 - iou: 0.4120 - dice_coef: 0.5825 - val_loss: 0.1795 - val_acc: 0.9269 - val_recall: 0.6685 - val_precision: 0.5800 - val_iou: 0.3711 - val_dice_coef: 0.5408 - lr: 0.0050\nEpoch 21/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.1246 - acc: 0.9477 - recall: 0.6112 - precision: 0.8083 - iou: 0.4334 - dice_coef: 0.6037 - val_loss: 0.1916 - val_acc: 0.9163 - val_recall: 0.8323 - val_precision: 0.5223 - val_iou: 0.3683 - val_dice_coef: 0.5369 - lr: 0.0050\nEpoch 22/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.1185 - acc: 0.9505 - recall: 0.6403 - precision: 0.8161 - iou: 0.4527 - dice_coef: 0.6220 - val_loss: 0.2331 - val_acc: 0.9184 - val_recall: 0.7908 - val_precision: 0.5320 - val_iou: 0.3904 - val_dice_coef: 0.5604 - lr: 0.0050\nEpoch 23/100\n31/31 [==============================] - 17s 541ms/step - loss: 0.1038 - acc: 0.9564 - recall: 0.6844 - precision: 0.8452 - iou: 0.4965 - dice_coef: 0.6622 - val_loss: 0.1144 - val_acc: 0.9534 - val_recall: 0.7014 - val_precision: 0.7659 - val_iou: 0.4817 - val_dice_coef: 0.6495 - lr: 0.0025\nEpoch 24/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.0953 - acc: 0.9596 - recall: 0.7180 - precision: 0.8521 - iou: 0.5321 - dice_coef: 0.6931 - val_loss: 0.1068 - val_acc: 0.9589 - val_recall: 0.6502 - val_precision: 0.8614 - val_iou: 0.5027 - val_dice_coef: 0.6683 - lr: 0.0025\nEpoch 25/100\n31/31 [==============================] - 16s 524ms/step - loss: 0.0891 - acc: 0.9618 - recall: 0.7362 - precision: 0.8608 - iou: 0.5560 - dice_coef: 0.7133 - val_loss: 0.1051 - val_acc: 0.9591 - val_recall: 0.6276 - val_precision: 0.8855 - val_iou: 0.5090 - val_dice_coef: 0.6739 - lr: 0.0025\nEpoch 26/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0836 - acc: 0.9640 - recall: 0.7545 - precision: 0.8682 - iou: 0.5771 - dice_coef: 0.7305 - val_loss: 0.1066 - val_acc: 0.9596 - val_recall: 0.6349 - val_precision: 0.8854 - val_iou: 0.5127 - val_dice_coef: 0.6774 - lr: 0.0025\nEpoch 27/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.0796 - acc: 0.9654 - recall: 0.7666 - precision: 0.8731 - iou: 0.5925 - dice_coef: 0.7428 - val_loss: 0.1029 - val_acc: 0.9611 - val_recall: 0.6782 - val_precision: 0.8626 - val_iou: 0.5342 - val_dice_coef: 0.6961 - lr: 0.0025\nEpoch 28/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0781 - acc: 0.9660 - recall: 0.7724 - precision: 0.8751 - iou: 0.5989 - dice_coef: 0.7476 - val_loss: 0.0985 - val_acc: 0.9632 - val_recall: 0.7231 - val_precision: 0.8509 - val_iou: 0.5676 - val_dice_coef: 0.7239 - lr: 0.0025\nEpoch 29/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0758 - acc: 0.9671 - recall: 0.7820 - precision: 0.8784 - iou: 0.6098 - dice_coef: 0.7561 - val_loss: 0.0976 - val_acc: 0.9632 - val_recall: 0.7484 - val_precision: 0.8342 - val_iou: 0.5770 - val_dice_coef: 0.7313 - lr: 0.0025\nEpoch 30/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0728 - acc: 0.9682 - recall: 0.7891 - precision: 0.8840 - iou: 0.6228 - dice_coef: 0.7660 - val_loss: 0.1013 - val_acc: 0.9618 - val_recall: 0.6849 - val_precision: 0.8631 - val_iou: 0.5585 - val_dice_coef: 0.7161 - lr: 0.0025\nEpoch 31/100\n31/31 [==============================] - 17s 541ms/step - loss: 0.0726 - acc: 0.9680 - recall: 0.7881 - precision: 0.8822 - iou: 0.6209 - dice_coef: 0.7646 - val_loss: 0.1131 - val_acc: 0.9601 - val_recall: 0.6074 - val_precision: 0.9217 - val_iou: 0.5331 - val_dice_coef: 0.6940 - lr: 0.0025\nEpoch 32/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.0654 - acc: 0.9708 - recall: 0.8099 - precision: 0.8930 - iou: 0.6493 - dice_coef: 0.7862 - val_loss: 0.1128 - val_acc: 0.9615 - val_recall: 0.6262 - val_precision: 0.9199 - val_iou: 0.5518 - val_dice_coef: 0.7095 - lr: 0.0025\nEpoch 33/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0644 - acc: 0.9711 - recall: 0.8111 - precision: 0.8956 - iou: 0.6547 - dice_coef: 0.7899 - val_loss: 0.1016 - val_acc: 0.9621 - val_recall: 0.6775 - val_precision: 0.8758 - val_iou: 0.5718 - val_dice_coef: 0.7269 - lr: 0.0025\nEpoch 34/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.0662 - acc: 0.9705 - recall: 0.8085 - precision: 0.8913 - iou: 0.6515 - dice_coef: 0.7875 - val_loss: 0.1074 - val_acc: 0.9574 - val_recall: 0.8296 - val_precision: 0.7398 - val_iou: 0.5679 - val_dice_coef: 0.7245 - lr: 0.0025\nEpoch 35/100\n31/31 [==============================] - 16s 525ms/step - loss: 0.0589 - acc: 0.9736 - recall: 0.8271 - precision: 0.9069 - iou: 0.6731 - dice_coef: 0.8029 - val_loss: 0.0799 - val_acc: 0.9677 - val_recall: 0.7412 - val_precision: 0.8854 - val_iou: 0.6080 - val_dice_coef: 0.7559 - lr: 0.0012\nEpoch 36/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0484 - acc: 0.9778 - recall: 0.8583 - precision: 0.9237 - iou: 0.7219 - dice_coef: 0.8375 - val_loss: 0.0766 - val_acc: 0.9702 - val_recall: 0.7790 - val_precision: 0.8821 - val_iou: 0.6455 - val_dice_coef: 0.7844 - lr: 0.0012\nEpoch 37/100\n31/31 [==============================] - 16s 524ms/step - loss: 0.0423 - acc: 0.9802 - recall: 0.8782 - precision: 0.9322 - iou: 0.7505 - dice_coef: 0.8567 - val_loss: 0.0780 - val_acc: 0.9706 - val_recall: 0.7913 - val_precision: 0.8770 - val_iou: 0.6570 - val_dice_coef: 0.7928 - lr: 0.0012\nEpoch 38/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0377 - acc: 0.9819 - recall: 0.8907 - precision: 0.9385 - iou: 0.7731 - dice_coef: 0.8714 - val_loss: 0.0780 - val_acc: 0.9708 - val_recall: 0.8055 - val_precision: 0.8690 - val_iou: 0.6677 - val_dice_coef: 0.8005 - lr: 0.0012\nEpoch 39/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0344 - acc: 0.9831 - recall: 0.9012 - precision: 0.9419 - iou: 0.7902 - dice_coef: 0.8822 - val_loss: 0.0783 - val_acc: 0.9712 - val_recall: 0.8152 - val_precision: 0.8659 - val_iou: 0.6761 - val_dice_coef: 0.8066 - lr: 0.0012\nEpoch 40/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0319 - acc: 0.9841 - recall: 0.9079 - precision: 0.9461 - iou: 0.8051 - dice_coef: 0.8915 - val_loss: 0.0793 - val_acc: 0.9716 - val_recall: 0.8261 - val_precision: 0.8631 - val_iou: 0.6905 - val_dice_coef: 0.8167 - lr: 0.0012\nEpoch 41/100\n31/31 [==============================] - 16s 524ms/step - loss: 0.0296 - acc: 0.9848 - recall: 0.9127 - precision: 0.9494 - iou: 0.8172 - dice_coef: 0.8990 - val_loss: 0.0805 - val_acc: 0.9717 - val_recall: 0.8175 - val_precision: 0.8716 - val_iou: 0.6963 - val_dice_coef: 0.8207 - lr: 0.0012\nEpoch 42/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0284 - acc: 0.9851 - recall: 0.9172 - precision: 0.9483 - iou: 0.8224 - dice_coef: 0.9021 - val_loss: 0.1083 - val_acc: 0.9671 - val_recall: 0.6926 - val_precision: 0.9261 - val_iou: 0.6362 - val_dice_coef: 0.7766 - lr: 6.2500e-04\nEpoch 43/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0277 - acc: 0.9854 - recall: 0.9173 - precision: 0.9518 - iou: 0.8273 - dice_coef: 0.9052 - val_loss: 0.0806 - val_acc: 0.9740 - val_recall: 0.7860 - val_precision: 0.9227 - val_iou: 0.7118 - val_dice_coef: 0.8314 - lr: 6.2500e-04\nEpoch 44/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0239 - acc: 0.9870 - recall: 0.9251 - precision: 0.9607 - iou: 0.8485 - dice_coef: 0.9179 - val_loss: 0.0876 - val_acc: 0.9727 - val_recall: 0.7628 - val_precision: 0.9274 - val_iou: 0.6974 - val_dice_coef: 0.8214 - lr: 6.2500e-04\nEpoch 45/100\n31/31 [==============================] - 16s 523ms/step - loss: 0.0219 - acc: 0.9878 - recall: 0.9308 - precision: 0.9634 - iou: 0.8595 - dice_coef: 0.9243 - val_loss: 0.0845 - val_acc: 0.9736 - val_recall: 0.7833 - val_precision: 0.9206 - val_iou: 0.7115 - val_dice_coef: 0.8312 - lr: 6.2500e-04\nEpoch 46/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0207 - acc: 0.9882 - recall: 0.9339 - precision: 0.9651 - iou: 0.8666 - dice_coef: 0.9284 - val_loss: 0.0880 - val_acc: 0.9732 - val_recall: 0.7794 - val_precision: 0.9195 - val_iou: 0.7095 - val_dice_coef: 0.8299 - lr: 6.2500e-04\nEpoch 47/100\n31/31 [==============================] - 16s 524ms/step - loss: 0.0197 - acc: 0.9885 - recall: 0.9369 - precision: 0.9659 - iou: 0.8721 - dice_coef: 0.9316 - val_loss: 0.0800 - val_acc: 0.9757 - val_recall: 0.8152 - val_precision: 0.9158 - val_iou: 0.7359 - val_dice_coef: 0.8479 - lr: 3.1250e-04\nEpoch 48/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0195 - acc: 0.9886 - recall: 0.9364 - precision: 0.9672 - iou: 0.8738 - dice_coef: 0.9326 - val_loss: 0.0794 - val_acc: 0.9760 - val_recall: 0.8225 - val_precision: 0.9139 - val_iou: 0.7404 - val_dice_coef: 0.8508 - lr: 3.1250e-04\nEpoch 49/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0184 - acc: 0.9890 - recall: 0.9386 - precision: 0.9694 - iou: 0.8798 - dice_coef: 0.9360 - val_loss: 0.0807 - val_acc: 0.9759 - val_recall: 0.8177 - val_precision: 0.9163 - val_iou: 0.7403 - val_dice_coef: 0.8507 - lr: 3.1250e-04\nEpoch 50/100\n31/31 [==============================] - 17s 545ms/step - loss: 0.0176 - acc: 0.9893 - recall: 0.9401 - precision: 0.9714 - iou: 0.8848 - dice_coef: 0.9388 - val_loss: 0.0816 - val_acc: 0.9760 - val_recall: 0.8186 - val_precision: 0.9158 - val_iou: 0.7423 - val_dice_coef: 0.8520 - lr: 3.1250e-04\nEpoch 51/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0169 - acc: 0.9896 - recall: 0.9413 - precision: 0.9727 - iou: 0.8888 - dice_coef: 0.9411 - val_loss: 0.0823 - val_acc: 0.9760 - val_recall: 0.8199 - val_precision: 0.9152 - val_iou: 0.7442 - val_dice_coef: 0.8533 - lr: 3.1250e-04\nEpoch 52/100\n31/31 [==============================] - 17s 541ms/step - loss: 0.0163 - acc: 0.9898 - recall: 0.9435 - precision: 0.9732 - iou: 0.8920 - dice_coef: 0.9429 - val_loss: 0.0806 - val_acc: 0.9762 - val_recall: 0.8287 - val_precision: 0.9096 - val_iou: 0.7476 - val_dice_coef: 0.8555 - lr: 1.5625e-04\nEpoch 53/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0160 - acc: 0.9899 - recall: 0.9431 - precision: 0.9747 - iou: 0.8942 - dice_coef: 0.9441 - val_loss: 0.0805 - val_acc: 0.9763 - val_recall: 0.8354 - val_precision: 0.9055 - val_iou: 0.7511 - val_dice_coef: 0.8578 - lr: 1.5625e-04\nEpoch 54/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0156 - acc: 0.9900 - recall: 0.9436 - precision: 0.9755 - iou: 0.8960 - dice_coef: 0.9451 - val_loss: 0.0809 - val_acc: 0.9763 - val_recall: 0.8395 - val_precision: 0.9026 - val_iou: 0.7530 - val_dice_coef: 0.8590 - lr: 1.5625e-04\nEpoch 55/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0154 - acc: 0.9901 - recall: 0.9441 - precision: 0.9761 - iou: 0.8977 - dice_coef: 0.9460 - val_loss: 0.0816 - val_acc: 0.9762 - val_recall: 0.8414 - val_precision: 0.9006 - val_iou: 0.7540 - val_dice_coef: 0.8597 - lr: 1.5625e-04\nEpoch 56/100\n31/31 [==============================] - 17s 541ms/step - loss: 0.0151 - acc: 0.9902 - recall: 0.9446 - precision: 0.9765 - iou: 0.8992 - dice_coef: 0.9469 - val_loss: 0.0822 - val_acc: 0.9762 - val_recall: 0.8428 - val_precision: 0.8992 - val_iou: 0.7547 - val_dice_coef: 0.8602 - lr: 1.5625e-04\nEpoch 57/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0148 - acc: 0.9903 - recall: 0.9441 - precision: 0.9779 - iou: 0.9009 - dice_coef: 0.9478 - val_loss: 0.0823 - val_acc: 0.9760 - val_recall: 0.8520 - val_precision: 0.8910 - val_iou: 0.7556 - val_dice_coef: 0.8607 - lr: 7.8125e-05\nEpoch 58/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0146 - acc: 0.9904 - recall: 0.9454 - precision: 0.9777 - iou: 0.9019 - dice_coef: 0.9484 - val_loss: 0.0829 - val_acc: 0.9760 - val_recall: 0.8522 - val_precision: 0.8907 - val_iou: 0.7560 - val_dice_coef: 0.8610 - lr: 7.8125e-05\nEpoch 59/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0145 - acc: 0.9905 - recall: 0.9459 - precision: 0.9778 - iou: 0.9027 - dice_coef: 0.9488 - val_loss: 0.0834 - val_acc: 0.9759 - val_recall: 0.8527 - val_precision: 0.8900 - val_iou: 0.7565 - val_dice_coef: 0.8613 - lr: 7.8125e-05\nEpoch 60/100\n31/31 [==============================] - 16s 522ms/step - loss: 0.0144 - acc: 0.9905 - recall: 0.9460 - precision: 0.9781 - iou: 0.9034 - dice_coef: 0.9492 - val_loss: 0.0839 - val_acc: 0.9759 - val_recall: 0.8536 - val_precision: 0.8893 - val_iou: 0.7568 - val_dice_coef: 0.8615 - lr: 7.8125e-05\nEpoch 61/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0143 - acc: 0.9905 - recall: 0.9462 - precision: 0.9783 - iou: 0.9041 - dice_coef: 0.9496 - val_loss: 0.0844 - val_acc: 0.9759 - val_recall: 0.8541 - val_precision: 0.8887 - val_iou: 0.7572 - val_dice_coef: 0.8618 - lr: 7.8125e-05\nEpoch 62/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0141 - acc: 0.9906 - recall: 0.9462 - precision: 0.9788 - iou: 0.9050 - dice_coef: 0.9501 - val_loss: 0.0848 - val_acc: 0.9759 - val_recall: 0.8556 - val_precision: 0.8872 - val_iou: 0.7574 - val_dice_coef: 0.8619 - lr: 3.9062e-05\nEpoch 63/100\n31/31 [==============================] - 16s 521ms/step - loss: 0.0141 - acc: 0.9906 - recall: 0.9467 - precision: 0.9787 - iou: 0.9054 - dice_coef: 0.9503 - val_loss: 0.0851 - val_acc: 0.9758 - val_recall: 0.8563 - val_precision: 0.8866 - val_iou: 0.7576 - val_dice_coef: 0.8621 - lr: 3.9062e-05\nEpoch 64/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0140 - acc: 0.9906 - recall: 0.9467 - precision: 0.9788 - iou: 0.9057 - dice_coef: 0.9505 - val_loss: 0.0854 - val_acc: 0.9758 - val_recall: 0.8569 - val_precision: 0.8859 - val_iou: 0.7578 - val_dice_coef: 0.8622 - lr: 3.9062e-05\nEpoch 65/100\n31/31 [==============================] - 17s 539ms/step - loss: 0.0140 - acc: 0.9907 - recall: 0.9469 - precision: 0.9789 - iou: 0.9061 - dice_coef: 0.9507 - val_loss: 0.0856 - val_acc: 0.9758 - val_recall: 0.8573 - val_precision: 0.8854 - val_iou: 0.7580 - val_dice_coef: 0.8623 - lr: 3.9062e-05\nEpoch 66/100\n31/31 [==============================] - 17s 540ms/step - loss: 0.0139 - acc: 0.9907 - recall: 0.9469 - precision: 0.9790 - iou: 0.9064 - dice_coef: 0.9509 - val_loss: 0.0859 - val_acc: 0.9758 - val_recall: 0.8576 - val_precision: 0.8850 - val_iou: 0.7581 - val_dice_coef: 0.8624 - lr: 3.9062e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_image_new(path):\n    \n    x = cv2.imread(path , cv2.IMREAD_COLOR)\n    x = cv2.resize(x , (256, 256))\n    x = x/255.0\n    # size is 256, 256 , 3\n    return x\n\n\ndef read_mask_new(path):\n\n    x = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x , (256, 256))\n    # size is 256, 256\n    x = np.expand_dims(x, axis = -1)\n    # size is 256, 256 , 1\n    \n    return x\n\ndef mask_parse(mask):\n    \n    mask = np.squeeze(mask)\n    mask = [mask , mask , mask]\n    \n    mask = np.transpose(mask , (1,2,0))\n    return mask","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:00.666228Z","iopub.execute_input":"2023-08-31T20:11:00.666554Z","iopub.status.idle":"2023-08-31T20:11:00.674872Z","shell.execute_reply.started":"2023-08-31T20:11:00.666525Z","shell.execute_reply":"2023-08-31T20:11:00.673864Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom tensorflow.keras.utils import CustomObjectScope\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:00.676558Z","iopub.execute_input":"2023-08-31T20:11:00.677347Z","iopub.status.idle":"2023-08-31T20:11:00.695222Z","shell.execute_reply.started":"2023-08-31T20:11:00.677309Z","shell.execute_reply":"2023-08-31T20:11:00.694091Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    test_dataset = tf_dataset(test_x , test_y , batch = batch)\n    test_steps = len(test_x) // batch\n    \n    if len(test_x)%batch != 0 :\n        test_steps +=1\n        \n    with CustomObjectScope({'iou' : iou , 'dice_coef' : dice_coef ,  'dice_coef_loss' :dice_coef_loss }):\n    \n        model = tf.keras.models.load_model(\"/kaggle/working/model.h5\")\n        \n    model.evaluate(test_dataset , steps = test_steps)\n    \n    for i , (x,y) in tqdm(enumerate(zip(test_x , test_y)) , total= len(test_x)):\n        \n        x = read_image_new(x)\n        y = read_mask_new(y)\n        \n        y_pred = model.predict(np.expand_dims(x , axis = 0))\n        \n        y_pred= y_pred[0] > 0.5\n        \n        h,w, _ = x.shape\n        \n        white_line = np.ones((h,10,3)) * 255.0\n        \n        all_images = [\n            \n            x*255.0, white_line,\n            \n            mask_parse(y) , white_line,\n            mask_parse(y_pred)*255.0\n            \n        ]\n        \n        image = np.concatenate (all_images, axis =1)\n        cv2.imwrite(f\"/kaggle/working/{i}.png\" , image)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:00.696471Z","iopub.execute_input":"2023-08-31T20:11:00.696973Z","iopub.status.idle":"2023-08-31T20:11:10.762349Z","shell.execute_reply.started":"2023-08-31T20:11:00.696923Z","shell.execute_reply":"2023-08-31T20:11:10.761369Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 2s 320ms/step - loss: 0.1020 - acc: 0.9710 - recall: 0.8239 - precision: 0.8919 - iou: 0.7376 - dice_coef: 0.8487\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 1/61 [00:01<01:46,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 22ms/step\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 3/61 [00:01<00:30,  1.89it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 5/61 [00:02<00:16,  3.35it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 11%|█▏        | 7/61 [00:02<00:11,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▍        | 9/61 [00:02<00:08,  6.27it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 11/61 [00:02<00:06,  7.52it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 21%|██▏       | 13/61 [00:02<00:05,  8.65it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 15/61 [00:02<00:04,  9.49it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 17/61 [00:03<00:04, 10.09it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 19/61 [00:03<00:03, 10.59it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 21/61 [00:03<00:03, 11.24it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 23/61 [00:03<00:03, 11.69it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 25/61 [00:03<00:03, 11.80it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 26ms/step\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 27/61 [00:03<00:02, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 29/61 [00:04<00:02, 12.20it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 31/61 [00:04<00:02, 12.41it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 33/61 [00:04<00:02, 12.40it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 35/61 [00:04<00:02, 12.33it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 37/61 [00:04<00:01, 12.52it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 39/61 [00:04<00:01, 12.24it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 41/61 [00:05<00:01, 12.12it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 43/61 [00:05<00:02,  8.45it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 45/61 [00:05<00:01,  9.25it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 47/61 [00:05<00:01,  9.94it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 49/61 [00:05<00:01, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▎ | 51/61 [00:06<00:00, 10.85it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 53/61 [00:06<00:00, 11.02it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 55/61 [00:06<00:00, 11.21it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 57/61 [00:06<00:00, 11.44it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 59/61 [00:06<00:00, 11.76it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 61/61 [00:06<00:00,  8.73it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:10.764133Z","iopub.execute_input":"2023-08-31T20:11:10.764805Z","iopub.status.idle":"2023-08-31T20:11:10.772592Z","shell.execute_reply.started":"2023-08-31T20:11:10.764768Z","shell.execute_reply":"2023-08-31T20:11:10.771343Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(256, 256, 1)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r segmentation_model.zip  \"/kaggle/input/segmentation-model\"","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:10.774200Z","iopub.execute_input":"2023-08-31T20:11:10.774551Z","iopub.status.idle":"2023-08-31T20:11:10.782866Z","shell.execute_reply.started":"2023-08-31T20:11:10.774518Z","shell.execute_reply":"2023-08-31T20:11:10.781907Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!zip -r attention_cvc_new.zip   \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:11:10.784610Z","iopub.execute_input":"2023-08-31T20:11:10.785217Z","iopub.status.idle":"2023-08-31T20:11:17.104443Z","shell.execute_reply.started":"2023-08-31T20:11:10.785182Z","shell.execute_reply":"2023-08-31T20:11:17.103285Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/30.png (deflated 5%)\n  adding: kaggle/working/26.png (deflated 6%)\n  adding: kaggle/working/9.png (deflated 6%)\n  adding: kaggle/working/48.png (deflated 7%)\n  adding: kaggle/working/28.png (deflated 5%)\n  adding: kaggle/working/4.png (deflated 6%)\n  adding: kaggle/working/model.h5 (deflated 8%)\n  adding: kaggle/working/16.png (deflated 7%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/41.png (deflated 6%)\n  adding: kaggle/working/36.png (deflated 8%)\n  adding: kaggle/working/0.png (deflated 6%)\n  adding: kaggle/working/40.png (deflated 5%)\n  adding: kaggle/working/3.png (deflated 5%)\n  adding: kaggle/working/17.png (deflated 5%)\n  adding: kaggle/working/50.png (deflated 6%)\n  adding: kaggle/working/32.png (deflated 9%)\n  adding: kaggle/working/20.png (deflated 7%)\n  adding: kaggle/working/14.png (deflated 7%)\n  adding: kaggle/working/49.png (deflated 7%)\n  adding: kaggle/working/13.png (deflated 5%)\n  adding: kaggle/working/15.png (deflated 5%)\n  adding: kaggle/working/27.png (deflated 7%)\n  adding: kaggle/working/55.png (deflated 6%)\n  adding: kaggle/working/37.png (deflated 9%)\n  adding: kaggle/working/39.png (deflated 5%)\n  adding: kaggle/working/19.png (deflated 7%)\n  adding: kaggle/working/54.png (deflated 6%)\n  adding: kaggle/working/33.png (deflated 5%)\n  adding: kaggle/working/18.png (deflated 6%)\n  adding: kaggle/working/56.png (deflated 8%)\n  adding: kaggle/working/44.png (deflated 5%)\n  adding: kaggle/working/58.png (deflated 8%)\n  adding: kaggle/working/25.png (deflated 7%)\n  adding: kaggle/working/24.png (deflated 6%)\n  adding: kaggle/working/29.png (deflated 7%)\n  adding: kaggle/working/23.png (deflated 6%)\n  adding: kaggle/working/5.png (deflated 6%)\n  adding: kaggle/working/42.png (deflated 9%)\n  adding: kaggle/working/8.png (deflated 8%)\n  adding: kaggle/working/38.png (deflated 7%)\n  adding: kaggle/working/35.png (deflated 6%)\n  adding: kaggle/working/21.png (deflated 6%)\n  adding: kaggle/working/60.png (deflated 7%)\n  adding: kaggle/working/46.png (deflated 5%)\n  adding: kaggle/working/12.png (deflated 6%)\n  adding: kaggle/working/59.png (deflated 5%)\n  adding: kaggle/working/51.png (deflated 9%)\n  adding: kaggle/working/11.png (deflated 7%)\n  adding: kaggle/working/45.png (deflated 5%)\n  adding: kaggle/working/34.png (deflated 7%)\n  adding: kaggle/working/57.png (deflated 6%)\n  adding: kaggle/working/52.png (deflated 7%)\n  adding: kaggle/working/47.png (deflated 5%)\n  adding: kaggle/working/10.png (deflated 6%)\n  adding: kaggle/working/22.png (deflated 5%)\n  adding: kaggle/working/1.png (deflated 7%)\n  adding: kaggle/working/6.png (deflated 6%)\n  adding: kaggle/working/7.png (deflated 5%)\n  adding: kaggle/working/31.png (deflated 7%)\n  adding: kaggle/working/43.png (deflated 7%)\n  adding: kaggle/working/2.png (deflated 6%)\n  adding: kaggle/working/53.png (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}