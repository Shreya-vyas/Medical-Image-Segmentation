{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# data loading \n\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Recall, Precision\n\nH = 256\nW = 256\n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\ndef shuffling(x, y):\n    x, y = shuffle(x, y, random_state=42)\n    return x, y\n\ndef load_data(dataset_path):\n    split = 0.1\n    images = sorted(glob(os.path.join(dataset_path, \"trainx\", \"*.bmp\")))\n    masks = sorted(glob(os.path.join(dataset_path, \"trainy\", \"*.bmp\")))\n\n    test_size = int(len(images) * split)\n\n    train_x, valid_x = train_test_split(images, test_size=test_size, random_state=42)\n    train_y, valid_y = train_test_split(masks, test_size=test_size, random_state=42)\n\n    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-01T12:13:22.905781Z","iopub.execute_input":"2023-09-01T12:13:22.906179Z","iopub.status.idle":"2023-09-01T12:13:22.918110Z","shell.execute_reply.started":"2023-09-01T12:13:22.906147Z","shell.execute_reply":"2023-09-01T12:13:22.917017Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)  ## (H, W, 3)\n    x = cv2.resize(x, (W, H))\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x                                ## (256, 256, 3)\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (H, W)\n    x = cv2.resize(x, (W, H))\n    x = x/255.0\n    x = x.astype(np.float32)                    ## (256, 256)\n    x = np.expand_dims(x, axis=-1)              ## (256, 256, 1)\n    return x\n\ndef tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n    x.set_shape([H, W, 3])\n    y.set_shape([H, W, 1])\n    return x, y\n\ndef tf_dataset(X, Y, batch):\n    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(10)\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:13:22.924140Z","iopub.execute_input":"2023-09-01T12:13:22.925159Z","iopub.status.idle":"2023-09-01T12:13:22.937273Z","shell.execute_reply.started":"2023-09-01T12:13:22.925123Z","shell.execute_reply":"2023-09-01T12:13:22.936218Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# metrics \n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\nsmooth = 1e-15\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:13:22.939795Z","iopub.execute_input":"2023-09-01T12:13:22.940312Z","iopub.status.idle":"2023-09-01T12:13:22.952093Z","shell.execute_reply.started":"2023-09-01T12:13:22.940280Z","shell.execute_reply":"2023-09-01T12:13:22.950987Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# unet model\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.models import Model\n\ndef conv_block(inputs, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef encoder_block(inputs, num_filters):\n    x = conv_block(inputs, num_filters)\n    p = MaxPool2D((2, 2))(x)\n    return x, p\n\ndef decoder_block(inputs, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_unet(input_shape):\n    inputs = Input(input_shape)\n\n    \"\"\" Encoder \"\"\"\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n    s4, p4 = encoder_block(p3, 512)\n\n    \"\"\" Bridge \"\"\"\n    b1 = conv_block(p4, 1024)\n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n\n    \"\"\" Outputs \"\"\"\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n\n    \"\"\" Model \"\"\"\n    model = Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:13:22.953937Z","iopub.execute_input":"2023-09-01T12:13:22.954384Z","iopub.status.idle":"2023-09-01T12:13:22.967260Z","shell.execute_reply.started":"2023-09-01T12:13:22.954353Z","shell.execute_reply":"2023-09-01T12:13:22.966216Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    \"\"\" Seeding \"\"\"\n    np.random.seed(42)\n    tf.random.set_seed(42)\n\n    \"\"\" Folder for saving data \"\"\"\n    create_dir(\"files\")\n\n    \"\"\" Hyperparameters \"\"\"\n    batch_size = 4\n    lr = 1e-5 ## (0.0001)\n    num_epoch = 100\n    model_path = \"files/model.h5\"\n    csv_path = \"files/data.csv\"\n\n    dataset_path = \"/kaggle/input/ph2-resized\"\n    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n\n    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n\n    train_dataset = tf_dataset(train_x, train_y, batch_size)\n    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n\n    train_steps = len(train_x)//batch_size\n    valid_steps = len(valid_x)//batch_size\n\n    if len(train_x) % batch_size != 0:\n        train_steps += 1\n\n    if len(valid_x) % batch_size != 0:\n        valid_steps += 1\n\n    \"\"\" Model \"\"\"\n    model = build_unet((H, W, 3))\n    metrics = [dice_coef, iou, Recall(), Precision()]\n    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr), metrics=metrics)\n    model.summary()\n\n    callbacks = [\n        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n        CSVLogger(csv_path),\n        TensorBoard(),\n        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n    ]\n\n    model.fit(\n        train_dataset,\n        epochs=num_epoch,\n        validation_data=valid_dataset,\n        steps_per_epoch=train_steps,\n        validation_steps=valid_steps,\n        callbacks=callbacks\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:13:22.968569Z","iopub.execute_input":"2023-09-01T12:13:22.968974Z","iopub.status.idle":"2023-09-01T12:18:24.451201Z","shell.execute_reply.started":"2023-09-01T12:13:22.968943Z","shell.execute_reply":"2023-09-01T12:18:24.450284Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Train: 160 - 160\nValid: 20 - 20\nTest: 20 - 20\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_38 (Conv2D)             (None, 256, 256, 64  1792        ['input_3[0][0]']                \n                                )                                                                 \n                                                                                                  \n batch_normalization_36 (BatchN  (None, 256, 256, 64  256        ['conv2d_38[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_36 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_36[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_39 (Conv2D)             (None, 256, 256, 64  36928       ['activation_36[0][0]']          \n                                )                                                                 \n                                                                                                  \n batch_normalization_37 (BatchN  (None, 256, 256, 64  256        ['conv2d_39[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_37 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_37[0][0]'] \n                                )                                                                 \n                                                                                                  \n max_pooling2d_8 (MaxPooling2D)  (None, 128, 128, 64  0          ['activation_37[0][0]']          \n                                )                                                                 \n                                                                                                  \n conv2d_40 (Conv2D)             (None, 128, 128, 12  73856       ['max_pooling2d_8[0][0]']        \n                                8)                                                                \n                                                                                                  \n batch_normalization_38 (BatchN  (None, 128, 128, 12  512        ['conv2d_40[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_38 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_38[0][0]'] \n                                8)                                                                \n                                                                                                  \n conv2d_41 (Conv2D)             (None, 128, 128, 12  147584      ['activation_38[0][0]']          \n                                8)                                                                \n                                                                                                  \n batch_normalization_39 (BatchN  (None, 128, 128, 12  512        ['conv2d_41[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_39 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_39[0][0]'] \n                                8)                                                                \n                                                                                                  \n max_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 128)  0          ['activation_39[0][0]']          \n                                                                                                  \n conv2d_42 (Conv2D)             (None, 64, 64, 256)  295168      ['max_pooling2d_9[0][0]']        \n                                                                                                  \n batch_normalization_40 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_42[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_40 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_40[0][0]'] \n                                                                                                  \n conv2d_43 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_40[0][0]']          \n                                                                                                  \n batch_normalization_41 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_43[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_41 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_41[0][0]'] \n                                                                                                  \n max_pooling2d_10 (MaxPooling2D  (None, 32, 32, 256)  0          ['activation_41[0][0]']          \n )                                                                                                \n                                                                                                  \n conv2d_44 (Conv2D)             (None, 32, 32, 512)  1180160     ['max_pooling2d_10[0][0]']       \n                                                                                                  \n batch_normalization_42 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_44[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_42 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_42[0][0]'] \n                                                                                                  \n conv2d_45 (Conv2D)             (None, 32, 32, 512)  2359808     ['activation_42[0][0]']          \n                                                                                                  \n batch_normalization_43 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_45[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_43 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_43[0][0]'] \n                                                                                                  \n max_pooling2d_11 (MaxPooling2D  (None, 16, 16, 512)  0          ['activation_43[0][0]']          \n )                                                                                                \n                                                                                                  \n conv2d_46 (Conv2D)             (None, 16, 16, 1024  4719616     ['max_pooling2d_11[0][0]']       \n                                )                                                                 \n                                                                                                  \n batch_normalization_44 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_46[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_44 (Activation)     (None, 16, 16, 1024  0           ['batch_normalization_44[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_47 (Conv2D)             (None, 16, 16, 1024  9438208     ['activation_44[0][0]']          \n                                )                                                                 \n                                                                                                  \n batch_normalization_45 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_47[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_45 (Activation)     (None, 16, 16, 1024  0           ['batch_normalization_45[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_transpose_8 (Conv2DTran  (None, 32, 32, 512)  2097664    ['activation_45[0][0]']          \n spose)                                                                                           \n                                                                                                  \n concatenate_8 (Concatenate)    (None, 32, 32, 1024  0           ['conv2d_transpose_8[0][0]',     \n                                )                                 'activation_43[0][0]']          \n                                                                                                  \n conv2d_48 (Conv2D)             (None, 32, 32, 512)  4719104     ['concatenate_8[0][0]']          \n                                                                                                  \n batch_normalization_46 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_48[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_46 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_46[0][0]'] \n                                                                                                  \n conv2d_49 (Conv2D)             (None, 32, 32, 512)  2359808     ['activation_46[0][0]']          \n                                                                                                  \n batch_normalization_47 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_49[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_47 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_47[0][0]'] \n                                                                                                  \n conv2d_transpose_9 (Conv2DTran  (None, 64, 64, 256)  524544     ['activation_47[0][0]']          \n spose)                                                                                           \n                                                                                                  \n concatenate_9 (Concatenate)    (None, 64, 64, 512)  0           ['conv2d_transpose_9[0][0]',     \n                                                                  'activation_41[0][0]']          \n                                                                                                  \n conv2d_50 (Conv2D)             (None, 64, 64, 256)  1179904     ['concatenate_9[0][0]']          \n                                                                                                  \n batch_normalization_48 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_50[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_48 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_48[0][0]'] \n                                                                                                  \n conv2d_51 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_48[0][0]']          \n                                                                                                  \n batch_normalization_49 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_51[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n activation_49 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_49[0][0]'] \n                                                                                                  \n conv2d_transpose_10 (Conv2DTra  (None, 128, 128, 12  131200     ['activation_49[0][0]']          \n nspose)                        8)                                                                \n                                                                                                  \n concatenate_10 (Concatenate)   (None, 128, 128, 25  0           ['conv2d_transpose_10[0][0]',    \n                                6)                                'activation_39[0][0]']          \n                                                                                                  \n conv2d_52 (Conv2D)             (None, 128, 128, 12  295040      ['concatenate_10[0][0]']         \n                                8)                                                                \n                                                                                                  \n batch_normalization_50 (BatchN  (None, 128, 128, 12  512        ['conv2d_52[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_50 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_50[0][0]'] \n                                8)                                                                \n                                                                                                  \n conv2d_53 (Conv2D)             (None, 128, 128, 12  147584      ['activation_50[0][0]']          \n                                8)                                                                \n                                                                                                  \n batch_normalization_51 (BatchN  (None, 128, 128, 12  512        ['conv2d_53[0][0]']              \n ormalization)                  8)                                                                \n                                                                                                  \n activation_51 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_51[0][0]'] \n                                8)                                                                \n                                                                                                  \n conv2d_transpose_11 (Conv2DTra  (None, 256, 256, 64  32832      ['activation_51[0][0]']          \n nspose)                        )                                                                 \n                                                                                                  \n concatenate_11 (Concatenate)   (None, 256, 256, 12  0           ['conv2d_transpose_11[0][0]',    \n                                8)                                'activation_37[0][0]']          \n                                                                                                  \n conv2d_54 (Conv2D)             (None, 256, 256, 64  73792       ['concatenate_11[0][0]']         \n                                )                                                                 \n                                                                                                  \n batch_normalization_52 (BatchN  (None, 256, 256, 64  256        ['conv2d_54[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_52 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_52[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_55 (Conv2D)             (None, 256, 256, 64  36928       ['activation_52[0][0]']          \n                                )                                                                 \n                                                                                                  \n batch_normalization_53 (BatchN  (None, 256, 256, 64  256        ['conv2d_55[0][0]']              \n ormalization)                  )                                                                 \n                                                                                                  \n activation_53 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_53[0][0]'] \n                                )                                                                 \n                                                                                                  \n conv2d_56 (Conv2D)             (None, 256, 256, 1)  65          ['activation_53[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 31,055,297\nTrainable params: 31,043,521\nNon-trainable params: 11,776\n__________________________________________________________________________________________________\nEpoch 1/100\n40/40 [==============================] - ETA: 0s - loss: 0.5649 - dice_coef: 0.4290 - iou: 0.2772 - recall_2: 0.4215 - precision_2: 0.6103\nEpoch 1: val_loss improved from inf to 0.66968, saving model to files/model.h5\n40/40 [==============================] - 22s 171ms/step - loss: 0.5649 - dice_coef: 0.4290 - iou: 0.2772 - recall_2: 0.4215 - precision_2: 0.6103 - val_loss: 0.6697 - val_dice_coef: 0.4081 - val_iou: 0.2573 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 2/100\n40/40 [==============================] - ETA: 0s - loss: 0.4029 - dice_coef: 0.5767 - iou: 0.4089 - recall_2: 0.7298 - precision_2: 0.7735\nEpoch 2: val_loss improved from 0.66968 to 0.64610, saving model to files/model.h5\n40/40 [==============================] - 6s 158ms/step - loss: 0.4029 - dice_coef: 0.5767 - iou: 0.4089 - recall_2: 0.7298 - precision_2: 0.7735 - val_loss: 0.6461 - val_dice_coef: 0.3947 - val_iou: 0.2465 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 3/100\n40/40 [==============================] - ETA: 0s - loss: 0.3334 - dice_coef: 0.6489 - iou: 0.4845 - recall_2: 0.7820 - precision_2: 0.8378\nEpoch 3: val_loss improved from 0.64610 to 0.63643, saving model to files/model.h5\n40/40 [==============================] - 6s 157ms/step - loss: 0.3334 - dice_coef: 0.6489 - iou: 0.4845 - recall_2: 0.7820 - precision_2: 0.8378 - val_loss: 0.6364 - val_dice_coef: 0.3714 - val_iou: 0.2285 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 4/100\n40/40 [==============================] - ETA: 0s - loss: 0.2855 - dice_coef: 0.6992 - iou: 0.5423 - recall_2: 0.8185 - precision_2: 0.8748\nEpoch 4: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.2855 - dice_coef: 0.6992 - iou: 0.5423 - recall_2: 0.8185 - precision_2: 0.8748 - val_loss: 0.6490 - val_dice_coef: 0.3351 - val_iou: 0.2015 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 5/100\n40/40 [==============================] - ETA: 0s - loss: 0.2490 - dice_coef: 0.7351 - iou: 0.5864 - recall_2: 0.8480 - precision_2: 0.8986\nEpoch 5: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.2490 - dice_coef: 0.7351 - iou: 0.5864 - recall_2: 0.8480 - precision_2: 0.8986 - val_loss: 0.7263 - val_dice_coef: 0.2680 - val_iou: 0.1548 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 6/100\n40/40 [==============================] - ETA: 0s - loss: 0.2171 - dice_coef: 0.7632 - iou: 0.6227 - recall_2: 0.8742 - precision_2: 0.9184\nEpoch 6: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.2171 - dice_coef: 0.7632 - iou: 0.6227 - recall_2: 0.8742 - precision_2: 0.9184 - val_loss: 0.8651 - val_dice_coef: 0.1899 - val_iou: 0.1050 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 7/100\n40/40 [==============================] - ETA: 0s - loss: 0.1869 - dice_coef: 0.7882 - iou: 0.6563 - recall_2: 0.8999 - precision_2: 0.9372\nEpoch 7: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.1869 - dice_coef: 0.7882 - iou: 0.6563 - recall_2: 0.8999 - precision_2: 0.9372 - val_loss: 0.9030 - val_dice_coef: 0.1740 - val_iou: 0.0953 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 8/100\n40/40 [==============================] - ETA: 0s - loss: 0.1604 - dice_coef: 0.8114 - iou: 0.6891 - recall_2: 0.9230 - precision_2: 0.9510\nEpoch 8: val_loss did not improve from 0.63643\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n40/40 [==============================] - 5s 130ms/step - loss: 0.1604 - dice_coef: 0.8114 - iou: 0.6891 - recall_2: 0.9230 - precision_2: 0.9510 - val_loss: 0.9392 - val_dice_coef: 0.1628 - val_iou: 0.0886 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-05\nEpoch 9/100\n40/40 [==============================] - ETA: 0s - loss: 0.1475 - dice_coef: 0.8219 - iou: 0.7045 - recall_2: 0.9280 - precision_2: 0.9592\nEpoch 9: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.1475 - dice_coef: 0.8219 - iou: 0.7045 - recall_2: 0.9280 - precision_2: 0.9592 - val_loss: 0.9575 - val_dice_coef: 0.1643 - val_iou: 0.0896 - val_recall_2: 5.1675e-04 - val_precision_2: 0.5916 - lr: 1.0000e-06\nEpoch 10/100\n40/40 [==============================] - ETA: 0s - loss: 0.1382 - dice_coef: 0.8289 - iou: 0.7149 - recall_2: 0.9378 - precision_2: 0.9643\nEpoch 10: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.1382 - dice_coef: 0.8289 - iou: 0.7149 - recall_2: 0.9378 - precision_2: 0.9643 - val_loss: 0.9086 - val_dice_coef: 0.2063 - val_iou: 0.1158 - val_recall_2: 0.0382 - val_precision_2: 0.9773 - lr: 1.0000e-06\nEpoch 11/100\n40/40 [==============================] - ETA: 0s - loss: 0.1352 - dice_coef: 0.8312 - iou: 0.7183 - recall_2: 0.9412 - precision_2: 0.9655\nEpoch 11: val_loss did not improve from 0.63643\n40/40 [==============================] - 5s 132ms/step - loss: 0.1352 - dice_coef: 0.8312 - iou: 0.7183 - recall_2: 0.9412 - precision_2: 0.9655 - val_loss: 0.7578 - val_dice_coef: 0.3164 - val_iou: 0.1931 - val_recall_2: 0.1247 - val_precision_2: 0.9817 - lr: 1.0000e-06\nEpoch 12/100\n40/40 [==============================] - ETA: 0s - loss: 0.1326 - dice_coef: 0.8334 - iou: 0.7216 - recall_2: 0.9434 - precision_2: 0.9666\nEpoch 12: val_loss improved from 0.63643 to 0.58329, saving model to files/model.h5\n40/40 [==============================] - 6s 156ms/step - loss: 0.1326 - dice_coef: 0.8334 - iou: 0.7216 - recall_2: 0.9434 - precision_2: 0.9666 - val_loss: 0.5833 - val_dice_coef: 0.4594 - val_iou: 0.3107 - val_recall_2: 0.2945 - val_precision_2: 0.9847 - lr: 1.0000e-06\nEpoch 13/100\n40/40 [==============================] - ETA: 0s - loss: 0.1302 - dice_coef: 0.8355 - iou: 0.7247 - recall_2: 0.9453 - precision_2: 0.9676\nEpoch 13: val_loss improved from 0.58329 to 0.43093, saving model to files/model.h5\n40/40 [==============================] - 6s 156ms/step - loss: 0.1302 - dice_coef: 0.8355 - iou: 0.7247 - recall_2: 0.9453 - precision_2: 0.9676 - val_loss: 0.4309 - val_dice_coef: 0.5968 - val_iou: 0.4340 - val_recall_2: 0.4844 - val_precision_2: 0.9803 - lr: 1.0000e-06\nEpoch 14/100\n40/40 [==============================] - ETA: 0s - loss: 0.1278 - dice_coef: 0.8376 - iou: 0.7277 - recall_2: 0.9470 - precision_2: 0.9686\nEpoch 14: val_loss improved from 0.43093 to 0.31994, saving model to files/model.h5\n40/40 [==============================] - 6s 158ms/step - loss: 0.1278 - dice_coef: 0.8376 - iou: 0.7277 - recall_2: 0.9470 - precision_2: 0.9686 - val_loss: 0.3199 - val_dice_coef: 0.7047 - val_iou: 0.5483 - val_recall_2: 0.6773 - val_precision_2: 0.9714 - lr: 1.0000e-06\nEpoch 15/100\n40/40 [==============================] - ETA: 0s - loss: 0.1256 - dice_coef: 0.8396 - iou: 0.7307 - recall_2: 0.9486 - precision_2: 0.9695\nEpoch 15: val_loss improved from 0.31994 to 0.26032, saving model to files/model.h5\n40/40 [==============================] - 6s 156ms/step - loss: 0.1256 - dice_coef: 0.8396 - iou: 0.7307 - recall_2: 0.9486 - precision_2: 0.9695 - val_loss: 0.2603 - val_dice_coef: 0.7635 - val_iou: 0.6202 - val_recall_2: 0.7791 - val_precision_2: 0.9495 - lr: 1.0000e-06\nEpoch 16/100\n40/40 [==============================] - ETA: 0s - loss: 0.1234 - dice_coef: 0.8415 - iou: 0.7336 - recall_2: 0.9502 - precision_2: 0.9704\nEpoch 16: val_loss improved from 0.26032 to 0.22980, saving model to files/model.h5\n40/40 [==============================] - 6s 157ms/step - loss: 0.1234 - dice_coef: 0.8415 - iou: 0.7336 - recall_2: 0.9502 - precision_2: 0.9704 - val_loss: 0.2298 - val_dice_coef: 0.7934 - val_iou: 0.6598 - val_recall_2: 0.8394 - val_precision_2: 0.9254 - lr: 1.0000e-06\nEpoch 17/100\n40/40 [==============================] - ETA: 0s - loss: 0.1213 - dice_coef: 0.8434 - iou: 0.7365 - recall_2: 0.9516 - precision_2: 0.9713\nEpoch 17: val_loss improved from 0.22980 to 0.21657, saving model to files/model.h5\n40/40 [==============================] - 6s 156ms/step - loss: 0.1213 - dice_coef: 0.8434 - iou: 0.7365 - recall_2: 0.9516 - precision_2: 0.9713 - val_loss: 0.2166 - val_dice_coef: 0.8087 - val_iou: 0.6815 - val_recall_2: 0.8728 - val_precision_2: 0.9019 - lr: 1.0000e-06\nEpoch 18/100\n40/40 [==============================] - ETA: 0s - loss: 0.1192 - dice_coef: 0.8453 - iou: 0.7393 - recall_2: 0.9530 - precision_2: 0.9722\nEpoch 18: val_loss improved from 0.21657 to 0.21300, saving model to files/model.h5\n40/40 [==============================] - 6s 158ms/step - loss: 0.1192 - dice_coef: 0.8453 - iou: 0.7393 - recall_2: 0.9530 - precision_2: 0.9722 - val_loss: 0.2130 - val_dice_coef: 0.8180 - val_iou: 0.6956 - val_recall_2: 0.9015 - val_precision_2: 0.8894 - lr: 1.0000e-06\nEpoch 19/100\n40/40 [==============================] - ETA: 0s - loss: 0.1172 - dice_coef: 0.8472 - iou: 0.7421 - recall_2: 0.9544 - precision_2: 0.9730\nEpoch 19: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1172 - dice_coef: 0.8472 - iou: 0.7421 - recall_2: 0.9544 - precision_2: 0.9730 - val_loss: 0.2154 - val_dice_coef: 0.8231 - val_iou: 0.7039 - val_recall_2: 0.9159 - val_precision_2: 0.8803 - lr: 1.0000e-06\nEpoch 20/100\n40/40 [==============================] - ETA: 0s - loss: 0.1152 - dice_coef: 0.8490 - iou: 0.7449 - recall_2: 0.9557 - precision_2: 0.9738\nEpoch 20: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 134ms/step - loss: 0.1152 - dice_coef: 0.8490 - iou: 0.7449 - recall_2: 0.9557 - precision_2: 0.9738 - val_loss: 0.2203 - val_dice_coef: 0.8256 - val_iou: 0.7081 - val_recall_2: 0.9212 - val_precision_2: 0.8725 - lr: 1.0000e-06\nEpoch 21/100\n40/40 [==============================] - ETA: 0s - loss: 0.1133 - dice_coef: 0.8508 - iou: 0.7476 - recall_2: 0.9569 - precision_2: 0.9745\nEpoch 21: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1133 - dice_coef: 0.8508 - iou: 0.7476 - recall_2: 0.9569 - precision_2: 0.9745 - val_loss: 0.2252 - val_dice_coef: 0.8269 - val_iou: 0.7105 - val_recall_2: 0.9240 - val_precision_2: 0.8663 - lr: 1.0000e-06\nEpoch 22/100\n40/40 [==============================] - ETA: 0s - loss: 0.1115 - dice_coef: 0.8525 - iou: 0.7502 - recall_2: 0.9582 - precision_2: 0.9752\nEpoch 22: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 133ms/step - loss: 0.1115 - dice_coef: 0.8525 - iou: 0.7502 - recall_2: 0.9582 - precision_2: 0.9752 - val_loss: 0.2295 - val_dice_coef: 0.8277 - val_iou: 0.7119 - val_recall_2: 0.9257 - val_precision_2: 0.8621 - lr: 1.0000e-06\nEpoch 23/100\n40/40 [==============================] - ETA: 0s - loss: 0.1097 - dice_coef: 0.8542 - iou: 0.7528 - recall_2: 0.9593 - precision_2: 0.9758\nEpoch 23: val_loss did not improve from 0.21300\n\nEpoch 23: ReduceLROnPlateau reducing learning rate to 1e-07.\n40/40 [==============================] - 5s 132ms/step - loss: 0.1097 - dice_coef: 0.8542 - iou: 0.7528 - recall_2: 0.9593 - precision_2: 0.9758 - val_loss: 0.2329 - val_dice_coef: 0.8283 - val_iou: 0.7131 - val_recall_2: 0.9267 - val_precision_2: 0.8594 - lr: 1.0000e-06\nEpoch 24/100\n40/40 [==============================] - ETA: 0s - loss: 0.1082 - dice_coef: 0.8554 - iou: 0.7546 - recall_2: 0.9605 - precision_2: 0.9768\nEpoch 24: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1082 - dice_coef: 0.8554 - iou: 0.7546 - recall_2: 0.9605 - precision_2: 0.9768 - val_loss: 0.2363 - val_dice_coef: 0.8284 - val_iou: 0.7134 - val_recall_2: 0.9270 - val_precision_2: 0.8572 - lr: 1.0000e-07\nEpoch 25/100\n40/40 [==============================] - ETA: 0s - loss: 0.1080 - dice_coef: 0.8556 - iou: 0.7550 - recall_2: 0.9606 - precision_2: 0.9767\nEpoch 25: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 133ms/step - loss: 0.1080 - dice_coef: 0.8556 - iou: 0.7550 - recall_2: 0.9606 - precision_2: 0.9767 - val_loss: 0.2386 - val_dice_coef: 0.8283 - val_iou: 0.7135 - val_recall_2: 0.9273 - val_precision_2: 0.8556 - lr: 1.0000e-07\nEpoch 26/100\n40/40 [==============================] - ETA: 0s - loss: 0.1078 - dice_coef: 0.8559 - iou: 0.7553 - recall_2: 0.9607 - precision_2: 0.9767\nEpoch 26: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1078 - dice_coef: 0.8559 - iou: 0.7553 - recall_2: 0.9607 - precision_2: 0.9767 - val_loss: 0.2399 - val_dice_coef: 0.8282 - val_iou: 0.7134 - val_recall_2: 0.9275 - val_precision_2: 0.8547 - lr: 1.0000e-07\nEpoch 27/100\n40/40 [==============================] - ETA: 0s - loss: 0.1076 - dice_coef: 0.8560 - iou: 0.7556 - recall_2: 0.9608 - precision_2: 0.9768\nEpoch 27: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1076 - dice_coef: 0.8560 - iou: 0.7556 - recall_2: 0.9608 - precision_2: 0.9768 - val_loss: 0.2408 - val_dice_coef: 0.8281 - val_iou: 0.7133 - val_recall_2: 0.9277 - val_precision_2: 0.8539 - lr: 1.0000e-07\nEpoch 28/100\n40/40 [==============================] - ETA: 0s - loss: 0.1074 - dice_coef: 0.8562 - iou: 0.7559 - recall_2: 0.9609 - precision_2: 0.9769\nEpoch 28: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1074 - dice_coef: 0.8562 - iou: 0.7559 - recall_2: 0.9609 - precision_2: 0.9769 - val_loss: 0.2414 - val_dice_coef: 0.8281 - val_iou: 0.7132 - val_recall_2: 0.9279 - val_precision_2: 0.8534 - lr: 1.0000e-07\nEpoch 29/100\n40/40 [==============================] - ETA: 0s - loss: 0.1072 - dice_coef: 0.8564 - iou: 0.7562 - recall_2: 0.9610 - precision_2: 0.9769\nEpoch 29: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 133ms/step - loss: 0.1072 - dice_coef: 0.8564 - iou: 0.7562 - recall_2: 0.9610 - precision_2: 0.9769 - val_loss: 0.2419 - val_dice_coef: 0.8280 - val_iou: 0.7132 - val_recall_2: 0.9280 - val_precision_2: 0.8530 - lr: 1.0000e-07\nEpoch 30/100\n40/40 [==============================] - ETA: 0s - loss: 0.1071 - dice_coef: 0.8566 - iou: 0.7564 - recall_2: 0.9611 - precision_2: 0.9770\nEpoch 30: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1071 - dice_coef: 0.8566 - iou: 0.7564 - recall_2: 0.9611 - precision_2: 0.9770 - val_loss: 0.2423 - val_dice_coef: 0.8280 - val_iou: 0.7132 - val_recall_2: 0.9282 - val_precision_2: 0.8526 - lr: 1.0000e-07\nEpoch 31/100\n40/40 [==============================] - ETA: 0s - loss: 0.1069 - dice_coef: 0.8567 - iou: 0.7567 - recall_2: 0.9612 - precision_2: 0.9770\nEpoch 31: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1069 - dice_coef: 0.8567 - iou: 0.7567 - recall_2: 0.9612 - precision_2: 0.9770 - val_loss: 0.2426 - val_dice_coef: 0.8280 - val_iou: 0.7132 - val_recall_2: 0.9283 - val_precision_2: 0.8524 - lr: 1.0000e-07\nEpoch 32/100\n40/40 [==============================] - ETA: 0s - loss: 0.1067 - dice_coef: 0.8569 - iou: 0.7570 - recall_2: 0.9613 - precision_2: 0.9771\nEpoch 32: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1067 - dice_coef: 0.8569 - iou: 0.7570 - recall_2: 0.9613 - precision_2: 0.9771 - val_loss: 0.2429 - val_dice_coef: 0.8281 - val_iou: 0.7133 - val_recall_2: 0.9284 - val_precision_2: 0.8522 - lr: 1.0000e-07\nEpoch 33/100\n40/40 [==============================] - ETA: 0s - loss: 0.1065 - dice_coef: 0.8571 - iou: 0.7572 - recall_2: 0.9614 - precision_2: 0.9772\nEpoch 33: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1065 - dice_coef: 0.8571 - iou: 0.7572 - recall_2: 0.9614 - precision_2: 0.9772 - val_loss: 0.2431 - val_dice_coef: 0.8281 - val_iou: 0.7134 - val_recall_2: 0.9284 - val_precision_2: 0.8521 - lr: 1.0000e-07\nEpoch 34/100\n40/40 [==============================] - ETA: 0s - loss: 0.1063 - dice_coef: 0.8573 - iou: 0.7575 - recall_2: 0.9616 - precision_2: 0.9772\nEpoch 34: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1063 - dice_coef: 0.8573 - iou: 0.7575 - recall_2: 0.9616 - precision_2: 0.9772 - val_loss: 0.2432 - val_dice_coef: 0.8282 - val_iou: 0.7135 - val_recall_2: 0.9284 - val_precision_2: 0.8520 - lr: 1.0000e-07\nEpoch 35/100\n40/40 [==============================] - ETA: 0s - loss: 0.1061 - dice_coef: 0.8575 - iou: 0.7578 - recall_2: 0.9617 - precision_2: 0.9773\nEpoch 35: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1061 - dice_coef: 0.8575 - iou: 0.7578 - recall_2: 0.9617 - precision_2: 0.9773 - val_loss: 0.2434 - val_dice_coef: 0.8283 - val_iou: 0.7136 - val_recall_2: 0.9285 - val_precision_2: 0.8519 - lr: 1.0000e-07\nEpoch 36/100\n40/40 [==============================] - ETA: 0s - loss: 0.1060 - dice_coef: 0.8576 - iou: 0.7581 - recall_2: 0.9618 - precision_2: 0.9774\nEpoch 36: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1060 - dice_coef: 0.8576 - iou: 0.7581 - recall_2: 0.9618 - precision_2: 0.9774 - val_loss: 0.2435 - val_dice_coef: 0.8283 - val_iou: 0.7137 - val_recall_2: 0.9285 - val_precision_2: 0.8519 - lr: 1.0000e-07\nEpoch 37/100\n40/40 [==============================] - ETA: 0s - loss: 0.1058 - dice_coef: 0.8578 - iou: 0.7583 - recall_2: 0.9619 - precision_2: 0.9774\nEpoch 37: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 130ms/step - loss: 0.1058 - dice_coef: 0.8578 - iou: 0.7583 - recall_2: 0.9619 - precision_2: 0.9774 - val_loss: 0.2436 - val_dice_coef: 0.8284 - val_iou: 0.7138 - val_recall_2: 0.9285 - val_precision_2: 0.8518 - lr: 1.0000e-07\nEpoch 38/100\n40/40 [==============================] - ETA: 0s - loss: 0.1056 - dice_coef: 0.8580 - iou: 0.7586 - recall_2: 0.9620 - precision_2: 0.9775\nEpoch 38: val_loss did not improve from 0.21300\n40/40 [==============================] - 5s 132ms/step - loss: 0.1056 - dice_coef: 0.8580 - iou: 0.7586 - recall_2: 0.9620 - precision_2: 0.9775 - val_loss: 0.2437 - val_dice_coef: 0.8285 - val_iou: 0.7139 - val_recall_2: 0.9285 - val_precision_2: 0.8518 - lr: 1.0000e-07\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n\n\nH = 256\nW = 256\n\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)  ## (H, W, 3)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x/255.0\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=0)\n    return ori_x, x                                ## (1, 256, 256, 3)\n\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (H, W)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x/255.0\n    x = x.astype(np.int32)                    ## (256, 256)\n    return ori_x, x\n\ndef save_results(ori_x, ori_y, y_pred, save_image_path):\n    line = np.ones((H, 10, 3)) * 255\n\n    ori_y = np.expand_dims(ori_y, axis=-1)  ## (256, 256, 1)\n    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1) ## (256, 256, 3)\n\n    y_pred = np.expand_dims(y_pred, axis=-1)  ## (256, 256, 1)\n    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) ## (256, 256, 3)\n\n    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred*255], axis=1)\n    cv2.imwrite(save_image_path, cat_images)\n\n\nif __name__ == \"__main__\":\n    \"\"\" Seeding \"\"\"\n    np.random.seed(42)\n    tf.random.set_seed(42)\n\n    \"\"\" Folder for saving results \"\"\"\n    create_dir(\"results\")\n\n    \"\"\" Load the model \"\"\"\n    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef}):\n        model = tf.keras.models.load_model(\"/kaggle/working/files/model.h5\")\n\n    \"\"\" Load the test data \"\"\"\n    dataset_path = \"/kaggle/input/ph2-resized\"\n    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n\n    SCORE = []\n    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n        \"\"\" Exctracting the image name \"\"\"\n        name = x.split(\"/\")[-1]\n\n        \"\"\" Read the image and mask \"\"\"\n        ori_x, x = read_image(x)\n        ori_y, y = read_mask(y)\n\n        \"\"\" Predicting the mask \"\"\"\n        y_pred = model.predict(x)[0] > 0.5\n        y_pred = np.squeeze(y_pred, axis=-1)\n        y_pred = y_pred.astype(np.int32)\n\n        \"\"\" Saving the predicted mask \"\"\"\n        save_image_path = f\"results/{name}\"\n        save_results(ori_x, ori_y, y_pred, save_image_path)\n\n        \"\"\" Flatten the array \"\"\"\n        y = y.flatten()\n        y_pred = y_pred.flatten()\n\n        \"\"\" Calculating metrics values \"\"\"\n        acc_value = accuracy_score(y, y_pred)\n        f1_value = f1_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        jac_value = jaccard_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        recall_value = recall_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        precision_value = precision_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n\n    \"\"\" mean metrics values \"\"\"\n    score = [s[1:] for s in SCORE]\n    score = np.mean(score, axis=0)\n    print(f\"Accuracy: {score[0]:0.5f}\")\n    print(f\"F1: {score[1]:0.5f}\")\n    print(f\"Jaccard: {score[2]:0.5f}\")\n    print(f\"Recall: {score[3]:0.5f}\")\n    print(f\"Precision: {score[4]:0.5f}\")\n\n    df = pd.DataFrame(SCORE, columns = [\"Image Name\", \"Acc\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n    df.to_csv(\"files/score.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:18:24.453702Z","iopub.execute_input":"2023-09-01T12:18:24.454289Z","iopub.status.idle":"2023-09-01T12:18:30.684600Z","shell.execute_reply.started":"2023-09-01T12:18:24.454253Z","shell.execute_reply":"2023-09-01T12:18:30.683502Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 404ms/step\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 1/20 [00:00<00:11,  1.69it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 22ms/step\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [00:00<00:06,  2.80it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 22ms/step\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [00:00<00:04,  3.49it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [00:01<00:03,  4.14it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 24ms/step\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [00:01<00:03,  4.42it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [00:01<00:03,  4.54it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [00:01<00:02,  4.82it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 8/20 [00:01<00:02,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 9/20 [00:02<00:02,  4.94it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 10/20 [00:02<00:01,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 11/20 [00:02<00:01,  5.15it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 12/20 [00:02<00:01,  4.92it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 13/20 [00:02<00:01,  4.77it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 14/20 [00:03<00:01,  4.82it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 23ms/step\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 15/20 [00:03<00:01,  4.55it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 16/20 [00:03<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 17/20 [00:03<00:00,  4.52it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 18/20 [00:04<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 19/20 [00:04<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:04<00:00,  4.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.94261\nF1: 0.88294\nJaccard: 0.81536\nRecall: 0.91479\nPrecision: 0.89619\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r unet_skin_lesion_newnew_last.zip   \"/kaggle/working/results\"","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:18:30.685980Z","iopub.execute_input":"2023-09-01T12:18:30.686624Z","iopub.status.idle":"2023-09-01T12:18:32.054160Z","shell.execute_reply.started":"2023-09-01T12:18:30.686590Z","shell.execute_reply":"2023-09-01T12:18:32.052995Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/results/ (stored 0%)\n  adding: kaggle/working/results/X_img_147.bmp (deflated 72%)\n  adding: kaggle/working/results/X_img_15.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_57.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_126.bmp (deflated 69%)\n  adding: kaggle/working/results/X_img_156.bmp (deflated 77%)\n  adding: kaggle/working/results/X_img_121.bmp (deflated 70%)\n  adding: kaggle/working/results/X_img_90.bmp (deflated 70%)\n  adding: kaggle/working/results/X_img_33.bmp (deflated 69%)\n  adding: kaggle/working/results/X_img_129.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_146.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_153.bmp (deflated 73%)\n  adding: kaggle/working/results/X_img_192.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_122.bmp (deflated 70%)\n  adding: kaggle/working/results/X_img_26.bmp (deflated 68%)\n  adding: kaggle/working/results/X_img_7.bmp (deflated 72%)\n  adding: kaggle/working/results/X_img_29.bmp (deflated 73%)\n  adding: kaggle/working/results/X_img_167.bmp (deflated 69%)\n  adding: kaggle/working/results/X_img_58.bmp (deflated 71%)\n  adding: kaggle/working/results/X_img_40.bmp (deflated 69%)\n  adding: kaggle/working/results/X_img_93.bmp (deflated 70%)\n","output_type":"stream"}]}]}